# Naive CUDA Matrix Multiplication

## 1. Objective
The first step was to implement a simple matrix multiplier, something i have done in C++ and torch for some previous works, so wasn't very difficult to implement. Took the basic LeetGPU format of code and implemented the basic formula needed.

Implemented a method to multiply any two matrices of dimensions $(A × B)$ and $(B × C)$, which can be formulated as below.

---

## 2. Mathematical Background

For two matrices **A** and **B** of size $N × M$ and $(M × L)$, their product **C = A × B** is defined as:

$$
C[i][j] = \sum_{k=0}^{M-1} A[i][k] \times B[k][j]
$$

Each element $C[i][j]$ represents the **dot product** of the $i^{th}$ row of **A** and the $j^{th}$ column of **B**.

---

## 3. Implementation Concept

In this, each thread is assigned to compute one element of the output matrix C.

The global thread coordinates are computed as below:

$$
\text{row} = \text{blockIdx.y} \times \text{blockDim.y} + \text{threadIdx.y}
$$

$$
\text{col} = \text{blockIdx.x} \times \text{blockDim.x} + \text{threadIdx.x}
$$

Since 2D matrices are flattned in GPU, this formula works.

### Problems faced: 

Switched order of x and y threads over rows and columns.

<img width="1325" height="72" alt="image" src="https://github.com/user-attachments/assets/a20d24f7-5e94-46c7-9a2a-d22f0627f643" />

Got this while even testing for performance after failing the multiplication task quite a few times.



---



